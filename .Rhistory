}
library(xgboost)
# xgboost1 denotes the xgboost model
xgboost1<-xgboost(data=data.matrix(train_data),
label=train_label,
max.depth = 11,
eta = 0.5,
nround = 25,
objective = "binary:logistic")
result <- xgboost1
if(export){
save(result, file = "../output/xgb_model.Rdata")
}
return(result)
}
GBM <- function(train_data, train_label, params){
if(!suppressWarnings(require('gbm')))
{
install.packages('gbm')
require('gbm')
}
library(gbm)
gbm<-gbm(train_label~.,
data = train_data,
distribution = "bernoulli",
n.trees = 1000,
shrinkage = 0.005,
interaction.depth = 6,
n.minobsinnode = 10,
keep.data = TRUE,
verbose = TRUE,
train.fraction = 0.5,
bag.fraction = 0.5,
cv.folds=3
)
best.iter2<-gbm.perf(adaboost1,method = "cv")
result <- list(model = gbm, params = best.iter2)
if(export){
save(result, file = "../output/gbm_model.Rdata")
}
return(result)
}
### Train with selected model
if(run.RF){
RF_model <- RF(train_data, train_label, params)
return(RF_model)
}
if(run.tf){
TF_model <- TF(train_data, train_label, params)
return(TF_model)
}
if(run.Ada){
Ada_model <- Ada(train_data, train_label, params)
return(Ada_model)
}
if(run.GBM){
GBM_model <- GBM(train_data, train_label, params)
return(GBM_model)
}
if(run.XGB){
XGB_model <- Xgb(train_data, train_label, params)
return(XGB_model)
}
if(run.SVM){
SVM_model <- SVM(train_data, train_label, params)
return(SVM_model)
}
}
train(train_data, train_label, params = 0, run.SVM = T)
train <- function(train_data, train_label, params, run.RF = F,run.tf = F,
run.Ada = F, run.GBM = F, run.XGB = F, run.SVM = F, export = T){
### Input:
###  -  processed features from images
###  -  labels
### Output: training model specification
### load libraries
library("gbm")
RF <- function(train_data, train_label, params){
require(randomForest, quietly = TRUE)
seed = 1
set.seed(seed)
rf_model <- tuneRF(train_data, train_label, ntreeTry = 500, doBest = TRUE)
if(export){
save(rf_model, file = "../output/rf_model.Rdata")
}
return(rf_model)
}
TF <- function(train_data, train_label, params){
save(tf_model, file = "../output/tf_model.Rdata")
return(TF_model)
}
SVM <- function(train_data, train_label, params){
library("e1071")
svm_model <- svm(train_label ~ ., data = train_data, type = "C-classification",
kernel = "radial", gamma = params)
return(svm_model)
}
Ada <- function(train_data, train_label, params){
if(!suppressWarnings(require('gbm')))
{
install.packages('gbm')
require('gbm')
}
library(gbm)
# adaboost1 denotes the adaboost model
adaboost1<-gbm(train_label~.,
data = train_data,
distribution = "adaboost",
n.trees = 1000,
shrinkage = 0.005,
interaction.depth = 6,
n.minobsinnode = 10,
keep.data = TRUE,
verbose = TRUE,
train.fraction = 0.5,
bag.fraction = 0.5,
cv.folds=3
)
# Using cross validation to find the best iteration number
best.iter2<-gbm.perf(adaboost1,method = "cv")
result <- list(model = adaboost1, params = best.iter2)
if(export){
save(result, file = "../output/ada_model.Rdata")
}
return(result)
}
Xgb <- function(train_data, train_label, params){
if(!suppressWarnings(require('xgboost')))
{
install.packages('xgboost')
require('xgboost')
}
library(xgboost)
# xgboost1 denotes the xgboost model
xgboost1<-xgboost(data=data.matrix(train_data),
label=train_label,
max.depth = 11,
eta = 0.5,
nround = 25,
objective = "binary:logistic")
result <- xgboost1
if(export){
save(result, file = "../output/xgb_model.Rdata")
}
return(result)
}
GBM <- function(train_data, train_label, params){
if(!suppressWarnings(require('gbm')))
{
install.packages('gbm')
require('gbm')
}
library(gbm)
gbm<-gbm(train_label~.,
data = train_data,
distribution = "bernoulli",
n.trees = 1000,
shrinkage = 0.005,
interaction.depth = 6,
n.minobsinnode = 10,
keep.data = TRUE,
verbose = TRUE,
train.fraction = 0.5,
bag.fraction = 0.5,
cv.folds=3
)
best.iter2<-gbm.perf(adaboost1,method = "cv")
result <- list(model = gbm, params = best.iter2)
if(export){
save(result, file = "../output/gbm_model.Rdata")
}
return(result)
}
### Train with selected model
if(run.RF){
RF_model <- RF(train_data, train_label, params)
return(RF_model)
}
if(run.tf){
TF_model <- TF(train_data, train_label, params)
return(TF_model)
}
if(run.Ada){
Ada_model <- Ada(train_data, train_label, params)
return(Ada_model)
}
if(run.GBM){
GBM_model <- GBM(train_data, train_label, params)
return(GBM_model)
}
if(run.XGB){
XGB_model <- Xgb(train_data, train_label, params)
return(XGB_model)
}
if(run.SVM){
SVM_model <- SVM(train_data, train_label, params)
return(SVM_model)
}
}
train(train_data, train_label, params = 0, run.SVM = T)
load("../data/split_data/train/train_label.Rdata")
load("../data/split_data/test/test_label.Rdata")
load("../data/split_data/train/train_HOG.Rdata")
load("../data/split_data/test/test_HOG.Rdata")
train_data<- train_HOG
test_data <- test_HOG
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
experiment_dir <- "/Users/wcheng/Desktop/project-2-/" # Modify this to where the pet images are located
img_train_dir  <- paste(experiment_dir, "data/train/", sep="")
img_test_dir   <- paste(experiment_dir, "data/test/", sep="")
sift_dir <- paste(experiment_dir,"data/train-features", sep = "")
run.feature.train <- F # process features for training set
run.test          <- T # run evaluation on an independent test set
run.feature.test  <- F # process features for test set
run.sift <- F
run.GBM <- F
run.RF = F
run.NN = F
run.Ada = F
run.XGB = T
run.SVM = F
load("../data/split_data/train/train_label.Rdata")
load("../data/split_data/test/test_label.Rdata")
tm_feature_train <- system.time(train_data <- feature(img_train_dir,
run.color = T,
run.LBP = T,
run.HOG = T,
export=T))
source("../lib/feature.R")
tm_feature_train <- system.time(train_data <- feature(img_train_dir,
run.color = T,
run.LBP = T,
run.HOG = T,
export=T))
tm_feature_train <- system.time(train_data <- feature(img_train_dir,
run.color = F,
run.LBP = T,
run.HOG = T,
export=T))
source("../lib/feature.R")
tm_feature_train <- system.time(train_data <- feature(img_train_dir,
run.color = F,
run.LBP = T,
run.HOG = T,
export=T))
tm_feature_train <- system.time(train_data <- feature(img_train_dir,
run.color = F,
run.LBP = F,
run.HOG = T,
export=T))
x <- c(5,2,5)
y <- c(5,1,5)
cor(x,y,"pearson")
cor(x,y,method = "Pearson")
cor(x,y,method = "pearson")
cor(x,y,method = "spearson")
cor(x,y,method = "spearsan")
cor(x,y,method = "spearman")
library("infotheo")
?mutinformation
mutinformation(rowA, rowB)
data       <- as.matrix(MS_UI)
rowA <- data[1,]
rowB <- data[2,]
load("./output/MS_UI.Rdata")
load("./output/MS_UI.Rdata")
setwd("/Users/wcheng/Desktop/Spring 2018/data science/project-3-algorithms-project-3-algorithms-group-2")
load("./output/MS_UI.Rdata")
load("./output/movie_UI.Rdata")
data       <- as.matrix(MS_UI)
rowA <- data[1,]
rowB <- data[2,]
library("infotheo")
mutinformation(rowA, rowB)
joint_values <- !is.na(rowA) & !is.na(rowB)
mutinformation(rowA[joint_values], rowB[joint_values])
mutinformation(rowA[joint_values], rowB[joint_values], method = "emp")
mutinformation(rowA[joint_values], rowB[joint_values], method = "shrink")
source("./lib/functions.R")
movie_UI         <- as.matrix(movie_UI)
movie_sim_weight <- matrix(NA, nrow = nrow(movie_UI), ncol = nrow(movie_UI))
movie_sim <- calc_weight(movie_UI, method = "entropy")
movie_sim <- calc_weight(movie_UI)
calc_weight(movie_UI, method = "entropy")
library("infotheo")
calc_weight(movie_UI, method = "entropy")
calc_weight(movie_UI, run.pearson = F, run.entropy = T)
weight_func <- function(rowA, rowB) {
# weight_func takes as input two rows (thought of as rows of the data matrix) and
# calculates the similarity between the two rows according to 'method'
joint_values <- !is.na(rowA) & !is.na(rowB)
if (sum(joint_values) == 0) {
return(0)
} else {
if (run.pearson) {
return(cor(rowA[joint_values], rowB[joint_values], method = 'pearson'))
}
if (run.entropy) {
library("infotheo")
return(mutinformation(rowA[joint_values], rowB[joint_values], method = 'emp'))
}
}
}
weight_func(rowA, rowB)
run.pearson = F
run.entropy = T
weight_func(rowA, rowB)
calc_weight <- function(data, run.pearson, run.entropy) {
## Calculate similarity weight matrix
##
## input: data   - movie data or MS data in user-item matrix form
##        method - 'pearson'
##
## output: similarity weight matrix
# Iniate the similarity weight matrix
data       <- as.matrix(data)
weight_mat <- matrix(NA, nrow = nrow(data), ncol = nrow(data))
weight_func <- function(rowA, rowB, run.pearson, run.entropy) {
# weight_func takes as input two rows (thought of as rows of the data matrix) and
# calculates the similarity between the two rows according to 'method'
joint_values <- !is.na(rowA) & !is.na(rowB)
if (sum(joint_values) == 0) {
return(0)
} else {
if (run.pearson) {
return(cor(rowA[joint_values], rowB[joint_values], method = 'pearson'))
}
if (run.entropy) {
library("infotheo")
return(mutinformation(rowA[joint_values], rowB[joint_values], method = 'emp'))
}
}
}
# Loops over the rows and calculate sall similarities using weight_func
for(i in 1:nrow(data)) {
weight_mat[i, ] <- apply(data, 1, weight_func, data[i, ])
print(i)
}
return(round(weight_mat, 4))
}
calc_weight(movie_UI, run.pearson = F, run.entropy = T)
calc_weight <- function(data, run.pearson, run.entropy) {
## Calculate similarity weight matrix
##
## input: data   - movie data or MS data in user-item matrix form
##        method - 'pearson'
##
## output: similarity weight matrix
# Iniate the similarity weight matrix
data       <- as.matrix(data)
weight_mat <- matrix(NA, nrow = nrow(data), ncol = nrow(data))
weight_func <- function(rowA, rowB) {
# weight_func takes as input two rows (thought of as rows of the data matrix) and
# calculates the similarity between the two rows according to 'method'
joint_values <- !is.na(rowA) & !is.na(rowB)
if (sum(joint_values) == 0) {
return(0)
} else {
if (run.pearson) {
return(cor(rowA[joint_values], rowB[joint_values], method = 'pearson'))
}
if (run.entropy) {
library("infotheo")
return(mutinformation(rowA[joint_values], rowB[joint_values], method = 'emp'))
}
}
}
# Loops over the rows and calculate sall similarities using weight_func
for(i in 1:nrow(data)) {
weight_mat[i, ] <- apply(data, 1, weight_func, data[i, ])
print(i)
}
return(round(weight_mat, 4))
}
calc_weight(movie_UI, run.pearson = F, run.entropy = T)
tm_movie_ent <- system.time(movie_ent <-
calc_weight(movie_UI, run.pearson = F, run.entropy = T))
calc_weight <- function(data, run.pearson, run.entropy, run.spearman, run.sqdiff) {
## Calculate similarity weight matrix
##
## input: data   - movie data or MS data in user-item matrix form
##        method - 'pearson'
##
## output: similarity weight matrix
# Iniate the similarity weight matrix
data       <- as.matrix(data)
weight_mat <- matrix(NA, nrow = nrow(data), ncol = nrow(data))
weight_func <- function(rowA, rowB) {
# weight_func takes as input two rows (thought of as rows of the data matrix) and
# calculates the similarity between the two rows according to 'method'
joint_values <- !is.na(rowA) & !is.na(rowB)
if (sum(joint_values) == 0) {
return(0)
} else {
if (run.pearson) {
return(cor(rowA[joint_values], rowB[joint_values], method = 'pearson'))
}
if (run.entropy) {
library("infotheo")
return(mutinformation(rowA[joint_values], rowB[joint_values], method = 'emp'))
}
if (run.spearman) {
return(cor(rowA[joint_values], rowB[joint_values], method = 'spearman'))
}
if (run.sqdiff) {
return(mean((rowA[joint_values]-rowB[joint_values])^2))
}
}
}
# Loops over the rows and calculate sall similarities using weight_func
for(i in 1:nrow(data)) {
weight_mat[i, ] <- apply(data, 1, weight_func, data[i, ])
print(i)
}
return(round(weight_mat, 4))
}
calc_weight(movie_UI, run.sqdiff = T)
calc_weight <- function(data, run.pearson=F, run.entropy=F, run.spearman=F, run.sqdiff=F) {
## Calculate similarity weight matrix
##
## input: data   - movie data or MS data in user-item matrix form
##        method - 'pearson'
##
## output: similarity weight matrix
# Iniate the similarity weight matrix
data       <- as.matrix(data)
weight_mat <- matrix(NA, nrow = nrow(data), ncol = nrow(data))
weight_func <- function(rowA, rowB) {
# weight_func takes as input two rows (thought of as rows of the data matrix) and
# calculates the similarity between the two rows according to 'method'
joint_values <- !is.na(rowA) & !is.na(rowB)
if (sum(joint_values) == 0) {
return(0)
} else {
if (run.pearson) {
return(cor(rowA[joint_values], rowB[joint_values], method = 'pearson'))
}
if (run.entropy) {
library("infotheo")
return(mutinformation(rowA[joint_values], rowB[joint_values], method = 'emp'))
}
if (run.spearman) {
return(cor(rowA[joint_values], rowB[joint_values], method = 'spearman'))
}
if (run.sqdiff) {
return(mean((rowA[joint_values]-rowB[joint_values])^2))
}
}
}
# Loops over the rows and calculate sall similarities using weight_func
for(i in 1:nrow(data)) {
weight_mat[i, ] <- apply(data, 1, weight_func, data[i, ])
print(i)
}
return(round(weight_mat, 4))
}
calc_weight(movie_UI, run.sqdiff = T)
calc_weight(movie_UI[1:10,], run.sqdiff = T)
calc_weight(movie_UI[1:10,], run.pearson = T)
calc_weight(movie_UI[1:10,], run.spearman = T)
calc_weight(movie_UI[1:10,], run.entropy = T)
if(!require("lsa")){
install.packages("lsa")
}
library("lsa")
install.packages("lsa")
library("lsa")
source("./lib/functions.R")
tm_movie_ent <- system.time(movie_ent <-
calc_weight(movie_UI, run.entropy = T))
tm_movie_sqd <- system.time(movie_sqd <-
calc_weight(movie_UI,run.sqdiff = T))
save(movie_sqd, file = "./output/movie_sqd.RData")
# Calculate the cosin weights on the MS data
# The below took  minutes
tm_MS_sqd <- system.time(MS_sqd <-
calc_weight(MS_UI, run.sqdiff = T))
save(MS_sqd, file = "./output/MS_sqd.RData")
tm_movie_sqd
tm_MS_sqd
tm_movie_ent <- system.time(movie_ent <-
calc_weight(movie_UI, run.entropy = T))
save(movie_ent, file = "./output/movie_ent.RData")
tm_MS_ent <- system.time(MS_ent <-
calc_weight(MS_UI, run.entropy = T))
tm_movie_ent
4256/29
View(movie_ent)
dim(movie_UI)
dim(MS_UI)
46460/5055
setwd("/Users/wcheng/Desktop/Spring 2018/data science/project-3-algorithms-project-3-algorithms-group-2")
load("./output/MS_UI.Rdata")
load("./output/movie_UI.Rdata")
source("./lib/functions.R")
tm_MS_ent <- system.time(MS_ent <-
calc_weight(MS_UI, run.entropy = T))
569/75
7.59*4151
tm_MS_ent <- system.time(MS_ent <-
calc_weight(MS_UI, run.entropy = T))
save(MS_ent, file = "./output/MS_ent.RData")
View(MS_ent)
tm_MS_ent
